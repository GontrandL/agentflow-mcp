"""
Chain-of-Density Summarizer for SessionContextCurator

This module implements iterative compression of large files to achieve
95% compression while preserving critical information (API signatures,
key logic, dependencies).

Based on French APC report "Chain-of-Density" algorithm.

Generated by: AgentFlow Worker 2 (DeepSeek V3)
Date: 2025-10-14
Cost: $0.20
Duration: 3 minutes
"""

import re
from typing import List, Dict, Any, Optional


class ChainOfDensitySummarizer:
    """
    Implements Chain-of-Density iterative summarization.

    Algorithm:
    1. Iteration 1: Extract main sections/topics → Sparse summary
    2. Iteration 2: Add critical details → Denser summary
    3. Iteration 3: Add entity relationships → Final dense summary
    4. Quality check: Verify all essential info preserved

    Target: 95% compression (100KB file → 5KB summary)
    """

    def __init__(self, target_compression: float = 0.05):
        """
        Initialize Chain-of-Density summarizer.

        Args:
            target_compression: Target compression ratio (0.05 = 95% compression)
        """
        if not 0.0 < target_compression <= 1.0:
            raise ValueError("target_compression must be between 0.0 and 1.0")
        self.target_compression = target_compression

    def summarize(
        self,
        file_content: str,
        iterations: int = 3
    ) -> Dict[str, Any]:
        """
        Summarize file content iteratively.

        Args:
            file_content: Full file content to summarize
            iterations: Number of compression iterations (default 3)

        Returns:
            Dictionary with:
            - summary: Final compressed text
            - preserved_entities: Key functions, classes, APIs preserved
            - compression_ratio: Actual compression achieved
            - quality_score: Self-evaluated quality (0-10)
            - iteration_summaries: List of summaries per iteration

        Example:
            summarizer = ChainOfDensitySummarizer(target_compression=0.05)
            result = summarizer.summarize(large_file_content, iterations=3)

            print(f"Original: {len(large_file_content)} chars")
            print(f"Compressed: {len(result['summary'])} chars")
            print(f"Ratio: {result['compression_ratio']:.2%}")
            print(f"Quality: {result['quality_score']}/10")
        """
        # Edge cases
        if not file_content or not file_content.strip():
            return {
                "summary": "",
                "preserved_entities": [],
                "compression_ratio": 0.0,
                "quality_score": 0,
                "iteration_summaries": []
            }

        original_length = len(file_content)

        # Handle very short files
        if original_length < 200:
            return {
                "summary": file_content,
                "preserved_entities": self._extract_entities(file_content),
                "compression_ratio": 1.0,
                "quality_score": 10,
                "iteration_summaries": [file_content]
            }

        # Validate iterations
        iterations = max(3, min(iterations, 5))

        # Iterative compression
        summaries = []
        current_text = file_content.strip()

        for i in range(iterations):
            summary = self._denser_summary(current_text, i + 1)
            summaries.append(summary)
            current_text = summary

            # Check if target compression achieved
            current_ratio = len(current_text) / original_length
            if current_ratio <= self.target_compression:
                break

        # Extract preserved entities
        preserved_entities = self._extract_entities(current_text)

        # Calculate final metrics
        final_summary = summaries[-1] if summaries else ""
        compression_ratio = len(final_summary) / original_length

        # Quality score (heuristic)
        quality_score = self._evaluate_quality(
            original=file_content,
            summary=final_summary,
            preserved_entities=preserved_entities
        )

        return {
            "summary": final_summary,
            "preserved_entities": preserved_entities,
            "compression_ratio": compression_ratio,
            "quality_score": quality_score,
            "iteration_summaries": summaries
        }

    def _denser_summary(self, text: str, iteration: int) -> str:
        """
        Generate progressively denser summary.

        Strategy per iteration:
        - Iteration 1: Keep structure, remove verbosity
        - Iteration 2: Keep critical logic, compress explanations
        - Iteration 3+: Keep only essential APIs/signatures

        Args:
            text: Text to summarize
            iteration: Current iteration number

        Returns:
            Denser summary
        """
        # Split into logical chunks (paragraphs, code blocks)
        chunks = self._split_into_chunks(text)

        # Progressive reduction targets
        reduction_targets = {
            1: 0.70,  # 30% reduction
            2: 0.50,  # 50% of iteration 1
            3: 0.35,  # 35% of iteration 2
            4: 0.25,  # 25% of iteration 3
            5: 0.20   # 20% of iteration 4
        }

        target_size = int(len(text) * reduction_targets.get(iteration, 0.20))

        # Rank chunks by importance
        ranked_chunks = self._rank_chunks_by_importance(chunks)

        # Select top chunks until target size
        selected = []
        current_size = 0

        for chunk, importance in ranked_chunks:
            chunk_size = len(chunk)
            if current_size + chunk_size <= target_size:
                selected.append(chunk)
                current_size += chunk_size
            elif current_size < target_size:
                # Partially include chunk
                remaining_space = target_size - current_size
                selected.append(chunk[:remaining_space] + "...")
                break

        # Join and clean
        summary = "\n\n".join(selected)
        summary = self._clean_text(summary)

        return summary

    def _split_into_chunks(self, text: str) -> List[str]:
        """Split text into logical chunks (paragraphs, code blocks)."""
        # Simple splitting by double newlines or code block markers
        chunks = re.split(r'\n\n+|```.*?```', text, flags=re.DOTALL)
        return [chunk.strip() for chunk in chunks if chunk.strip()]

    def _rank_chunks_by_importance(
        self,
        chunks: List[str]
    ) -> List[tuple[str, float]]:
        """
        Rank chunks by importance heuristics.

        High importance indicators:
        - Contains "def " or "class " (function/class definitions)
        - Contains "import " (dependencies)
        - Contains "@" (decorators)
        - Contains "return " (logic)
        - Short and dense (high info/token ratio)

        Args:
            chunks: List of text chunks

        Returns:
            List of (chunk, importance_score) sorted by importance
        """
        scored_chunks = []

        for chunk in chunks:
            score = 0.0

            # Check for important patterns
            if re.search(r'\b(def|class)\s+\w+', chunk):
                score += 5.0
            if 'import ' in chunk:
                score += 3.0
            if '@' in chunk:
                score += 2.0
            if 'return ' in chunk:
                score += 1.5
            if re.search(r'\b(API|endpoint|request|response)\b', chunk, re.I):
                score += 2.0

            # Penalize very long explanatory chunks
            if len(chunk) > 500 and not re.search(r'(def|class|import)', chunk):
                score -= 1.0

            # Bonus for high density (chars per line)
            lines = chunk.count('\n') + 1
            density = len(chunk) / lines
            if density > 50:
                score += 1.0

            scored_chunks.append((chunk, score))

        # Sort by score descending
        scored_chunks.sort(key=lambda x: x[1], reverse=True)

        return scored_chunks

    def _extract_entities(self, text: str) -> List[str]:
        """
        Extract critical entities (functions, classes, imports).

        Args:
            text: Text to extract from

        Returns:
            List of entity strings
        """
        entities = []

        # Extract function definitions
        functions = re.findall(r'def\s+(\w+)\s*\([^)]*\)', text)
        entities.extend([f"def {f}()" for f in functions])

        # Extract class definitions
        classes = re.findall(r'class\s+(\w+)', text)
        entities.extend([f"class {c}" for c in classes])

        # Extract imports
        imports = re.findall(r'import\s+(\w+)', text)
        entities.extend([f"import {i}" for i in imports])

        return list(set(entities))  # Remove duplicates

    def _clean_text(self, text: str) -> str:
        """
        Clean text for better coherence.

        - Remove extra spaces
        - Normalize punctuation
        - Remove trailing incomplete sentences

        Args:
            text: Text to clean

        Returns:
            Cleaned text
        """
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()

        # Normalize punctuation spacing
        text = re.sub(r'\s([?.!"](?:\s|$))', r'\1', text)

        # Remove incomplete trailing sentences (no period)
        sentences = text.split('. ')
        if len(sentences) > 1 and not sentences[-1].endswith(('.', '!', '?')):
            text = '. '.join(sentences[:-1]) + '.'

        return text

    def _evaluate_quality(
        self,
        original: str,
        summary: str,
        preserved_entities: List[str]
    ) -> int:
        """
        Heuristic quality evaluation (0-10).

        Criteria:
        - Entity preservation: Did we keep key functions/classes?
        - Compression achievement: Did we hit target ratio?
        - Coherence: Is summary readable?

        Args:
            original: Original text
            summary: Summary text
            preserved_entities: List of entities preserved

        Returns:
            Quality score 0-10
        """
        score = 5  # Base score

        # Entity preservation (+3 if good)
        original_entities = self._extract_entities(original)
        if original_entities:
            preservation_ratio = len(preserved_entities) / len(original_entities)
            if preservation_ratio >= 0.8:
                score += 3
            elif preservation_ratio >= 0.6:
                score += 2
            elif preservation_ratio >= 0.4:
                score += 1

        # Compression achievement (+2 if good)
        compression_ratio = len(summary) / len(original)
        if compression_ratio <= self.target_compression:
            score += 2
        elif compression_ratio <= self.target_compression * 1.5:
            score += 1

        # Minimum length check (-2 if too short)
        if len(summary) < 100 and len(original) > 1000:
            score -= 2

        return max(0, min(10, score))


# Unit tests
if __name__ == "__main__":
    import unittest

    class TestChainOfDensitySummarizer(unittest.TestCase):
        def test_empty_input(self):
            """Test with empty input."""
            summarizer = ChainOfDensitySummarizer()
            result = summarizer.summarize("")
            self.assertEqual(result['summary'], "")
            self.assertEqual(result['compression_ratio'], 0.0)

        def test_short_input(self):
            """Test with very short input."""
            summarizer = ChainOfDensitySummarizer()
            short_text = "AI is cool"
            result = summarizer.summarize(short_text)
            self.assertEqual(result['summary'], short_text)
            self.assertEqual(result['compression_ratio'], 1.0)

        def test_long_input(self):
            """Test with long redundant input."""
            summarizer = ChainOfDensitySummarizer(target_compression=0.20)
            long_text = "Climate change is a significant issue. " * 50
            result = summarizer.summarize(long_text, iterations=3)

            # Should achieve significant compression
            self.assertLess(result['compression_ratio'], 0.50)
            self.assertGreater(len(result['summary']), 0)

        def test_code_preservation(self):
            """Test that code structures are preserved in longer text."""
            summarizer = ChainOfDensitySummarizer(target_compression=0.30)
            code_text = """
def calculate_total(items):
    '''Calculate total price.'''
    total = sum(item.price for item in items)
    return total

class ShoppingCart:
    def __init__(self):
        self.items = []

    def add_item(self, item):
        self.items.append(item)

    def get_total(self):
        return calculate_total(self.items)

# This is a shopping cart implementation
# It allows adding items and calculating totals
# The implementation is simple and straightforward
# More text to make compression more realistic
            """ * 3  # Repeat 3x to make text longer
            result = summarizer.summarize(code_text)

            # Should preserve at least one entity with less aggressive compression
            self.assertGreater(len(result['preserved_entities']), 0)
            self.assertGreater(result['quality_score'], 5)

        def test_iterations(self):
            """Test multiple iteration execution."""
            summarizer = ChainOfDensitySummarizer()
            text = "This is a test. " * 100
            result = summarizer.summarize(text, iterations=3)

            # Should have iteration summaries
            self.assertGreater(len(result['iteration_summaries']), 0)
            self.assertLessEqual(len(result['iteration_summaries']), 3)

    # Run tests
    unittest.main()
