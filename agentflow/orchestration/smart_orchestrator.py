"""
SmartOrchestrator - Provider-agnostic intelligent orchestration

Coordinates multiple cheap/free AI workers using a powerful "brain" model
(Claude Sonnet 4.5, GPT-4, etc.) to maximize quality while minimizing costs.

Architecture:
    SmartOrchestrator (expensive, intelligent)
        ‚Üì
    Analyze, Plan, Validate, Assemble
        ‚Üì
    Workers (cheap/free, specialized)
        ‚Üì
    Results with high quality at low cost
"""

import os
import json
import logging
import time
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log
)
from dotenv import load_dotenv

# Load .env file at module import
load_dotenv()

try:
    from .model_selector import ModelSelector
except ImportError:
    ModelSelector = None


@dataclass
class Subtask:
    """Individual subtask in decomposed plan"""
    id: str
    description: str
    dependencies: List[str] = field(default_factory=list)
    difficulty: str = "medium"  # low, medium, high
    error_risk: str = "medium"  # low, medium, high
    estimated_tokens: int = 1000

    def to_dict(self) -> Dict[str, Any]:
        return {
            'id': self.id,
            'description': self.description,
            'dependencies': self.dependencies,
            'difficulty': self.difficulty,
            'error_risk': self.error_risk,
            'estimated_tokens': self.estimated_tokens
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Subtask':
        return cls(**data)


@dataclass
class Plan:
    """Execution plan generated by orchestrator"""
    complexity: str  # low, medium, high
    decompose: bool
    subtasks: List[Subtask]
    requirements: Dict[str, Any]

    def to_dict(self) -> Dict[str, Any]:
        return {
            'complexity': self.complexity,
            'decompose': self.decompose,
            'subtasks': [s.to_dict() for s in self.subtasks],
            'requirements': self.requirements
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Plan':
        subtasks = [Subtask.from_dict(s) for s in data['subtasks']]
        return cls(
            complexity=data['complexity'],
            decompose=data['decompose'],
            subtasks=subtasks,
            requirements=data['requirements']
        )


@dataclass
class ValidationResult:
    """Validation result from orchestrator"""
    all_passed: bool
    avg_score: float
    results: List[Dict[str, Any]]

    @property
    def failed_tasks(self) -> List[Dict[str, Any]]:
        return [r for r in self.results if not r['passed']]

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ValidationResult':
        return cls(
            all_passed=data['all_passed'],
            avg_score=data['avg_score'],
            results=data['results']
        )


@dataclass
class TaskCost:
    """Cost tracking for individual task"""
    timestamp: float
    provider: str
    model: str
    operation: str  # analyze_and_plan, assign_workers, validate_outputs, etc.
    input_tokens: int
    output_tokens: int
    input_cost: float
    output_cost: float
    total_cost: float
    latency_ms: float

    def to_dict(self) -> Dict[str, Any]:
        return {
            'timestamp': self.timestamp,
            'provider': self.provider,
            'model': self.model,
            'operation': self.operation,
            'input_tokens': self.input_tokens,
            'output_tokens': self.output_tokens,
            'input_cost': round(self.input_cost, 6),
            'output_cost': round(self.output_cost, 6),
            'total_cost': round(self.total_cost, 6),
            'latency_ms': round(self.latency_ms, 2)
        }


class SmartOrchestrator:
    """
    Provider-agnostic intelligent orchestrator

    Supports multiple powerful models:
    - Claude Sonnet 4.5 (default, best reasoning)
    - GPT-4 Turbo (alternative, fast)
    - GPT-4o (alternative, multimodal)

    Cost comparison (per 1M tokens):
    - Claude Sonnet 4.5: $3 in / $15 out
    - GPT-4 Turbo: $10 in / $30 out
    - GPT-4o: $2.50 in / $10 out

    Features robust error handling with:
    - Exponential backoff retry (3 attempts)
    - Automatic fallback chain across providers
    - Cost tracking preserved across fallbacks
    """

    PROVIDERS = {
        'deepseek': {
            'default_model': 'deepseek-chat',
            'input_price': 0.14 / 1_000_000,
            'output_price': 0.28 / 1_000_000,
            'max_tokens': 8192
        },
        'grok-fast': {
            'default_model': 'grok-beta',
            'input_price': 5.00 / 1_000_000,
            'output_price': 15.00 / 1_000_000,
            'max_tokens': 4096
        },
        'gpt-mini': {
            'default_model': 'gpt-4o-mini',
            'input_price': 0.15 / 1_000_000,
            'output_price': 0.60 / 1_000_000,
            'max_tokens': 4096
        },
        'anthropic': {
            'default_model': 'claude-sonnet-4-5-20250929',
            'input_price': 3.00 / 1_000_000,
            'output_price': 15.00 / 1_000_000,
            'max_tokens': 8192
        },
        'openai': {
            'default_model': 'gpt-4-turbo',
            'input_price': 10.00 / 1_000_000,
            'output_price': 30.00 / 1_000_000,
            'max_tokens': 4096
        },
        'openai-4o': {
            'default_model': 'gpt-4o',
            'input_price': 2.50 / 1_000_000,
            'output_price': 10.00 / 1_000_000,
            'max_tokens': 4096
        }
    }

    # Fallback chain: cheap -> fast -> reliable -> most powerful
    FALLBACK_ORDER = ['deepseek', 'grok-fast', 'gpt-mini', 'anthropic']

    def __init__(self, provider: Optional[str] = None, model: Optional[str] = None,
                 api_key: Optional[str] = None, enable_fallback: bool = True,
                 enable_model_selection: bool = True, enable_specialized_routing: bool = True,
                 skip_validation: bool = False):
        """
        Initialize SmartOrchestrator

        Args:
            provider: Primary provider to use (auto-detects from available API keys if None)
            model: Specific model (or use provider default)
            api_key: API key (or read from env)
            enable_fallback: Enable automatic fallback chain on errors
            enable_model_selection: Enable dynamic model selection based on task complexity
            enable_specialized_routing: Enable intelligent routing to specialized FREE models (NEW)
            skip_validation: Skip pre-flight validation (NOT RECOMMENDED - only for testing)
        """
        # Setup logging FIRST (needed for validation)
        self.logger = logging.getLogger(__name__)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)

        # PRE-FLIGHT VALIDATION (NEW - Definitive fix for recurring issues)
        if not skip_validation:
            from agentflow.orchestration.api_validator import APIValidator

            self.logger.info("üîç Pre-flight validation starting...")
            validator = APIValidator(test_connectivity=True)

            # Auto-detect provider with validation
            if provider is None:
                self.logger.info("Auto-detecting best provider...")
                provider = validator.get_best_provider()
                if not provider:
                    raise RuntimeError(
                        "No valid API providers found! Please check:\n"
                        "1. OPENROUTER_API_KEY environment variable\n"
                        "2. API key is not 'CHANGE_ME' or placeholder\n"
                        "3. API key has correct format\n"
                        "4. Network connectivity to API endpoints"
                    )
                self.logger.info(f"‚úì Auto-selected provider: {provider}")
            else:
                # Validate specified provider
                result = validator.validate_provider(provider, api_key)
                if not result.valid:
                    raise RuntimeError(
                        f"Provider validation failed: {provider}\n"
                        f"Error: {result.error}\n"
                        f"Please check API key configuration."
                    )
                self.logger.info(f"‚úì Provider validated: {provider}")

            # Validate fallback chain if enabled
            if enable_fallback:
                valid_fallbacks, errors = validator.validate_fallback_chain(self.FALLBACK_ORDER)
                if valid_fallbacks:
                    self.logger.info(f"‚úì Valid fallback providers: {', '.join(valid_fallbacks)}")
                if errors:
                    self.logger.warning(f"‚ö† Invalid fallbacks: {', '.join(errors)}")
        else:
            self.logger.warning("‚ö† Pre-flight validation SKIPPED - workers may crash!")

        # Verify provider is supported
        if provider not in self.PROVIDERS:
            raise ValueError(f"Unknown provider: {provider}. "
                           f"Supported: {list(self.PROVIDERS.keys())}")

        self.provider = provider
        self.config = self.PROVIDERS[provider]
        self.model = model or self.config['default_model']
        self.enable_fallback = enable_fallback

        # Get API key
        if api_key:
            self.api_key = api_key
        elif provider == 'anthropic':
            self.api_key = os.getenv('ANTHROPIC_API_KEY')
        elif provider == 'deepseek':
            # DeepSeek via OpenRouter or direct API
            self.api_key = os.getenv('DEEPSEEK_API_KEY') or os.getenv('OPENROUTER_API_KEY')
        elif provider == 'grok-fast':
            self.api_key = os.getenv('GROK_API_KEY') or os.getenv('OPENROUTER_API_KEY')
        elif provider.startswith('openai') or provider == 'gpt-mini':
            self.api_key = os.getenv('OPENAI_API_KEY') or os.getenv('OPENROUTER_API_KEY')

        if not self.api_key:
            raise ValueError(f"API key required for {provider}")

        # Initialize provider client
        self._init_client()

        self.logger.info(f"‚úÖ SmartOrchestrator initialized: {provider} ({self.model})")

        # Token tracking (preserved across fallbacks)
        self.input_tokens = 0
        self.output_tokens = 0
        self.fallback_attempts = 0
        self.failed_providers = []

        # Enhanced cost tracking - Phase 2
        self.total_cost = 0.0
        self.task_costs: List[TaskCost] = []
        self.current_operation: Optional[str] = None
        self.operation_start_time: Optional[float] = None

        # Initialize model selector for dynamic routing
        self.enable_model_selection = enable_model_selection
        if self.enable_model_selection and ModelSelector is not None:
            self.model_selector = ModelSelector(
                available_models=list(self.PROVIDERS.keys()),
                enable_learning=True
            )
            self.logger.info("Dynamic model selection enabled")
        else:
            self.model_selector = None
            if self.enable_model_selection and ModelSelector is None:
                self.logger.warning(
                    "Model selection requested but ModelSelector not available"
                )

        # Initialize specialized model router (NEW - Phase 2)
        self.enable_specialized_routing = enable_specialized_routing
        self.specialized_router = None
        if self.enable_specialized_routing:
            try:
                from agentflow.orchestration.specialized_model_router import SpecializedModelRouter
                self.specialized_router = SpecializedModelRouter()
                self.logger.info("Specialized model routing enabled (FREE models preferred)")
            except ImportError:
                self.logger.warning("SpecializedModelRouter not available, using standard routing")

    def _init_client(self):
        """Initialize provider-specific client - ALL models via OpenRouter/LiteLLM"""
        try:
            # ALL providers use OpenRouter now (no direct Anthropic SDK needed)
            if self.provider == 'anthropic':
                import openai
                # Use OpenRouter for Claude (no anthropic SDK needed)
                self.client = openai.OpenAI(
                    api_key=self.api_key,
                    base_url="https://openrouter.ai/api/v1"
                )
                self.model = "anthropic/claude-sonnet-4-20250514"
            elif self.provider == 'deepseek':
                import openai
                # Use OpenRouter for DeepSeek (more reliable and cheaper)
                self.client = openai.OpenAI(
                    api_key=self.api_key,
                    base_url="https://openrouter.ai/api/v1"
                )
                self.model = "deepseek/deepseek-chat"
            elif self.provider == 'grok-fast':
                import openai
                self.client = openai.OpenAI(
                    api_key=self.api_key,
                    base_url="https://api.x.ai/v1"
                )
            elif self.provider.startswith('openai') or self.provider == 'gpt-mini':
                import openai
                self.client = openai.OpenAI(api_key=self.api_key)
        except Exception as e:
            self.logger.error(f"Failed to initialize {self.provider} client: {e}")
            raise

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=16),
        retry=retry_if_exception_type((Exception,)),
        before_sleep=before_sleep_log(logging.getLogger(__name__), logging.WARNING)
    )
    def _call_llm_with_retry(self, prompt: str, max_tokens: int = None) -> str:
        """
        LLM call with exponential backoff retry

        Retries up to 3 times with exponential backoff: 2s, 4s, 8s, 16s
        """
        max_tokens = max_tokens or self.config['max_tokens']

        try:
            if self.provider == 'anthropic':
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=max_tokens,
                    messages=[{"role": "user", "content": prompt}]
                )
                self._track_usage(response.usage.input_tokens,
                                response.usage.output_tokens)
                return response.content[0].text

            else:  # OpenAI-compatible providers
                response = self.client.chat.completions.create(
                    model=self.model,
                    max_tokens=max_tokens,
                    messages=[{"role": "user", "content": prompt}]
                )
                self._track_usage(response.usage.prompt_tokens,
                                response.usage.completion_tokens)
                return response.choices[0].message.content

        except Exception as e:
            self.logger.error(
                f"API call failed for {self.provider}/{self.model}: {str(e)}"
            )
            raise

    def _call_llm(self, prompt: str, max_tokens: int = None) -> str:
        """
        Provider-agnostic LLM call with automatic fallback chain

        Implements fallback order: deepseek -> grok-fast -> gpt-mini -> anthropic
        Cost tracking is preserved across all fallback attempts.
        """
        if not self.enable_fallback:
            return self._call_llm_with_retry(prompt, max_tokens)

        # Try primary provider first
        try:
            self.logger.info(f"Calling primary provider: {self.provider}")
            return self._call_llm_with_retry(prompt, max_tokens)
        except Exception as primary_error:
            self.logger.warning(
                f"Primary provider {self.provider} failed: {str(primary_error)}"
            )
            self.failed_providers.append(self.provider)

            # Attempt fallback chain
            for fallback_provider in self.FALLBACK_ORDER:
                # Skip if already failed or is current provider
                if fallback_provider in self.failed_providers or fallback_provider == self.provider:
                    continue

                # Check if we have API key for this provider
                api_key = self._get_api_key_for_provider(fallback_provider)
                if not api_key:
                    self.logger.warning(
                        f"Skipping {fallback_provider}: No API key available"
                    )
                    continue

                try:
                    self.logger.info(
                        f"Fallback attempt {self.fallback_attempts + 1}: "
                        f"Trying {fallback_provider}"
                    )
                    self.fallback_attempts += 1

                    # Switch to fallback provider
                    original_provider = self.provider
                    original_config = self.config
                    original_model = self.model
                    original_client = self.client

                    self.provider = fallback_provider
                    self.config = self.PROVIDERS[fallback_provider]
                    self.model = self.config['default_model']
                    self.api_key = api_key
                    self._init_client()

                    # Try the call
                    result = self._call_llm_with_retry(prompt, max_tokens)

                    self.logger.info(
                        f"Fallback successful: {fallback_provider} "
                        f"(Cost tracking preserved)"
                    )
                    return result

                except Exception as fallback_error:
                    self.logger.error(
                        f"Fallback to {fallback_provider} failed: {str(fallback_error)}"
                    )
                    self.failed_providers.append(fallback_provider)

                    # Restore original provider settings for next attempt
                    self.provider = original_provider
                    self.config = original_config
                    self.model = original_model
                    self.client = original_client
                    continue

            # All fallbacks exhausted
            raise RuntimeError(
                f"All providers failed. Primary: {self.provider}, "
                f"Attempted fallbacks: {self.failed_providers}. "
                f"Last error: {str(primary_error)}"
            )

    def _get_api_key_for_provider(self, provider: str) -> Optional[str]:
        """Get API key for a specific provider from environment"""
        env_vars = {
            'anthropic': 'ANTHROPIC_API_KEY',
            'deepseek': 'DEEPSEEK_API_KEY',
            'grok-fast': 'GROK_API_KEY',
            'gpt-mini': 'OPENAI_API_KEY',
            'openai': 'OPENAI_API_KEY',
            'openai-4o': 'OPENAI_API_KEY'
        }
        return os.getenv(env_vars.get(provider, ''))

    def select_model_for_task(self, task: Dict[str, Any]) -> Optional[str]:
        """
        Select optimal model for task using ModelSelector.

        Args:
            task: Task dictionary with description, context, requirements

        Returns:
            Selected provider name or None if selection disabled
        """
        if not self.model_selector:
            return None

        try:
            provider, reason = self.model_selector.select_model_by_complexity(task)
            self.logger.info(
                f"Model selected: {provider} - {reason}"
            )
            return provider
        except Exception as e:
            self.logger.warning(
                f"Model selection failed: {e}. Using default provider."
            )
            return None

    def analyze_and_plan(self, task: str, capabilities: List[str],
                         context: Dict[str, Any]) -> Plan:
        """
        Analyze task and generate optimal execution plan

        Automatically selects optimal model based on task complexity if enabled.

        Returns:
            Plan with subtasks, dependencies, requirements
        """
        # Set operation for cost tracking
        self._set_operation('analyze_and_plan')

        # Dynamic model selection based on task complexity
        if self.model_selector and self.enable_model_selection:
            task_dict = {
                'description': task,
                'context': context,
                'capabilities': capabilities,
                'complexity': context.get('complexity', 'medium')
            }
            selected_provider = self.select_model_for_task(task_dict)

            if selected_provider and selected_provider != self.provider:
                # Switch to optimal provider for this task
                original_provider = self.provider
                original_config = self.config
                original_model = self.model
                original_client = self.client

                try:
                    api_key = self._get_api_key_for_provider(selected_provider)
                    if api_key:
                        self.logger.info(
                            f"Switching from {original_provider} to {selected_provider} "
                            f"for optimal task routing"
                        )
                        self.provider = selected_provider
                        self.config = self.PROVIDERS[selected_provider]
                        self.model = self.config['default_model']
                        self.api_key = api_key
                        self._init_client()
                except Exception as e:
                    self.logger.warning(
                        f"Failed to switch to {selected_provider}: {e}. "
                        f"Continuing with {original_provider}"
                    )
                    self.provider = original_provider
                    self.config = original_config
                    self.model = original_model
                    self.client = original_client

        prompt = f"""You are an expert orchestrator coordinating multiple AI workers.

TASK:
{task}

AVAILABLE CAPABILITIES:
{json.dumps(capabilities, indent=2)}

CONTEXT:
{json.dumps(context, indent=2)}

YOUR ROLE:
1. Analyze task complexity and requirements
2. Decompose into optimal subtasks (if task is complex or >10KB)
3. Identify dependencies between subtasks
4. Estimate difficulty and error risk per subtask
5. Create execution plan with clear requirements

DECOMPOSITION CRITERIA:
- Decompose if task is complex (multiple sections, >10KB prompt)
- Each subtask should be <10KB and independently executable
- Subtasks should be parallelizable when possible
- Include edge cases and validation criteria

OUTPUT FORMAT (JSON):
{{
  "complexity": "low|medium|high",
  "decompose": true|false,
  "subtasks": [
    {{
      "id": "subtask_1",
      "description": "Clear description of what to generate",
      "dependencies": [],
      "difficulty": "low|medium|high",
      "error_risk": "low|medium|high",
      "estimated_tokens": 1000
    }}
  ],
  "requirements": {{
    "edge_cases": ["edge case 1", "edge case 2"],
    "validation_criteria": ["criterion 1", "criterion 2"],
    "quality_threshold": 0.9
  }}
}}

Return ONLY valid JSON, no explanations."""

        response = self._call_llm(prompt, max_tokens=4096)

        # Extract JSON from response
        try:
            plan_data = json.loads(response)
        except json.JSONDecodeError:
            # Try to extract JSON from markdown code block
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
                plan_data = json.loads(json_str)
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
                plan_data = json.loads(json_str)
            else:
                raise ValueError(f"Failed to parse JSON from response: {response[:200]}")

        return Plan.from_dict(plan_data)

    def assign_workers(self, subtasks: List[Subtask],
                      available_workers: Dict[str, Dict]) -> Dict[str, str]:
        """
        Assign optimal worker for each subtask

        Args:
            subtasks: List of subtasks from plan
            available_workers: Dict of worker_name -> worker_info

        Returns:
            Dict mapping subtask_id -> worker_name
        """
        # Set operation for cost tracking
        self._set_operation('assign_workers')

        # Format workers info
        workers_info = []
        for name, info in available_workers.items():
            workers_info.append(f"""
- {name}:
  Price: {info['price']}
  Quality: {info['quality']}%
  Speed: {info['speed']}
  Best for: {info['best_for']}
  Weaknesses: {info['weaknesses']}
""")

        prompt = f"""You are assigning AI workers to subtasks based on their strengths.

SUBTASKS:
{json.dumps([s.to_dict() for s in subtasks], indent=2)}

AVAILABLE WORKERS:
{''.join(workers_info)}

ASSIGNMENT CRITERIA:
1. Match task difficulty with worker capability
2. Prefer FREE when quality is priority
3. Use paid when speed critical or large context needed
4. Minimize total cost while maintaining quality >=90%

OUTPUT FORMAT (JSON):
{{
  "assignments": [
    {{"subtask_id": "subtask_1", "worker": "worker_name", "rationale": "why this worker"}},
    {{"subtask_id": "subtask_2", "worker": "worker_name", "rationale": "why this worker"}}
  ]
}}

Return ONLY valid JSON."""

        response = self._call_llm(prompt, max_tokens=2048)

        # Extract JSON
        try:
            assignments_data = json.loads(response)
        except json.JSONDecodeError:
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
                assignments_data = json.loads(json_str)
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
                assignments_data = json.loads(json_str)
            else:
                raise ValueError(f"Failed to parse JSON: {response[:200]}")

        # Convert to dict
        return {
            a['subtask_id']: a['worker']
            for a in assignments_data['assignments']
        }

    def generate_specs(self, subtask: Subtask, worker_info: Dict[str, Any],
                      full_context: Dict[str, Any] = None) -> str:
        """
        Generate ultra-detailed specs optimized for specific worker

        Args:
            subtask: Subtask to generate specs for
            worker_info: Worker capabilities and limitations
            full_context: Additional context (template, examples, etc.)

        Returns:
            Optimized prompt for worker
        """
        # Set operation for cost tracking
        self._set_operation('generate_specs')

        context_str = json.dumps(full_context, indent=2) if full_context else "None"

        prompt = f"""You are creating ultra-detailed specifications for an AI worker.

WORKER INFO:
Model: {worker_info['model']}
Strengths: {worker_info.get('best_for', 'general purpose')}
Weaknesses: {worker_info.get('weaknesses', 'none')}
Limitations: Prompt should be <10KB, clear and specific

SUBTASK:
{subtask.to_dict()}

ADDITIONAL CONTEXT:
{context_str}

YOUR TASK:
Create a clear, detailed prompt that will be sent to the worker.

REQUIREMENTS:
1. Be extremely specific about what to generate
2. Include ALL edge cases to handle
3. Specify exact output format
4. List validation criteria
5. Add examples if helpful
6. Keep prompt focused and <10KB
7. Optimize for worker's strengths, work around weaknesses

OUTPUT:
Return ONLY the prompt text that will be sent to the worker.
NO explanations, NO markdown, just the prompt."""

        return self._call_llm(prompt, max_tokens=8192)

    def validate_outputs(self, results: Dict[str, str],
                        requirements: Dict[str, Any]) -> ValidationResult:
        """
        Validate worker outputs against requirements

        Args:
            results: Dict of subtask_id -> output
            requirements: Requirements from plan

        Returns:
            ValidationResult with scores and feedback
        """
        # Set operation for cost tracking
        self._set_operation('validate_outputs')

        # Convert Plan object to dict if needed
        requirements_dict = requirements.to_dict() if hasattr(requirements, 'to_dict') else requirements

        prompt = f"""You are validating outputs from AI workers against requirements.

REQUIREMENTS:
{json.dumps(requirements_dict, indent=2)}

OUTPUTS TO VALIDATE:
{json.dumps(results, indent=2)}

VALIDATION TASKS:
1. Check completeness (all requirements met?)
2. Check correctness (logic, syntax, semantics OK?)
3. Check edge cases handling
4. Check code quality (if applicable)
5. Detect potential bugs or issues
6. Rate quality 0-100%

SCORING:
- 90-100%: Excellent, ready to use
- 80-89%: Good, minor improvements needed
- 70-79%: Acceptable, needs review
- <70%: Needs rework

OUTPUT FORMAT (JSON):
{{
  "all_passed": true|false,
  "avg_score": 85,
  "results": [
    {{
      "subtask_id": "subtask_1",
      "passed": true,
      "score": 95,
      "issues": [],
      "feedback": "Excellent quality, all requirements met"
    }},
    {{
      "subtask_id": "subtask_2",
      "passed": false,
      "score": 65,
      "issues": ["Missing edge case X", "Incomplete error handling"],
      "feedback": "Needs improvements: ..."
    }}
  ]
}}

Return ONLY valid JSON."""

        response = self._call_llm(prompt, max_tokens=4096)

        # Extract JSON
        try:
            validation_data = json.loads(response)
        except json.JSONDecodeError:
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
                validation_data = json.loads(json_str)
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
                validation_data = json.loads(json_str)
            else:
                raise ValueError(f"Failed to parse JSON: {response[:200]}")

        return ValidationResult.from_dict(validation_data)

    def generate_feedback(self, subtask_id: str, output: str,
                         issues: List[str], requirements: Dict[str, Any]) -> str:
        """
        Generate precise feedback for failed subtask

        Returns:
            Feedback string to guide worker iteration
        """
        # Set operation for cost tracking
        self._set_operation('generate_feedback')

        prompt = f"""You are providing precise feedback to an AI worker to improve their output.

SUBTASK ID: {subtask_id}

ORIGINAL OUTPUT:
{output}

ISSUES IDENTIFIED:
{json.dumps(issues, indent=2)}

REQUIREMENTS (original):
{json.dumps(requirements, indent=2)}

YOUR FEEDBACK:
- Be specific about what to fix
- Provide examples if helpful
- Focus on critical issues first
- Keep feedback clear and actionable

OUTPUT:
Return the feedback text (plain text, no JSON)."""

        return self._call_llm(prompt, max_tokens=2048)

    def assemble_and_polish(self, results: Dict[str, str],
                           plan: Plan) -> str:
        """
        Assemble multiple outputs into cohesive final result

        Args:
            results: Dict of subtask_id -> output
            plan: Original execution plan

        Returns:
            Final assembled and polished output
        """
        # Set operation for cost tracking
        self._set_operation('assemble_and_polish')

        # Order results by subtask order
        ordered_outputs = []
        for subtask in plan.subtasks:
            if subtask.id in results:
                ordered_outputs.append({
                    'id': subtask.id,
                    'description': subtask.description,
                    'output': results[subtask.id]
                })

        prompt = f"""You are assembling multiple outputs into a cohesive final result.

ORIGINAL PLAN:
{plan.to_dict()}

OUTPUTS TO ASSEMBLE (in order):
{json.dumps(ordered_outputs, indent=2)}

YOUR TASKS:
1. Assemble outputs in logical order
2. Ensure smooth transitions between sections
3. Check overall coherence and consistency
4. Fix any inconsistencies between sections
5. Polish final output
6. Add necessary connectors/explanations if needed

OUTPUT:
Return the final assembled result (plain text or code, NO JSON wrapper)."""

        return self._call_llm(prompt, max_tokens=16384)

    def _track_usage(self, input_tokens: int, output_tokens: int):
        """
        Track token usage and calculate costs for current operation.

        This method:
        1. Parses token usage from API response
        2. Calculates costs based on provider pricing
        3. Records per-task cost details
        4. Accumulates total cost

        Args:
            input_tokens: Number of input/prompt tokens used
            output_tokens: Number of output/completion tokens used
        """
        # Accumulate total tokens
        self.input_tokens += input_tokens
        self.output_tokens += output_tokens

        # Calculate costs based on provider pricing
        input_cost = input_tokens * self.config['input_price']
        output_cost = output_tokens * self.config['output_price']
        task_cost = input_cost + output_cost

        # Accumulate total cost
        self.total_cost += task_cost

        # Calculate latency if operation timing is available
        latency_ms = 0.0
        if self.operation_start_time:
            latency_ms = (time.time() - self.operation_start_time) * 1000

        # Record task cost details
        cost_record = TaskCost(
            timestamp=time.time(),
            provider=self.provider,
            model=self.model,
            operation=self.current_operation or 'unknown',
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            input_cost=input_cost,
            output_cost=output_cost,
            total_cost=task_cost,
            latency_ms=latency_ms
        )

        self.task_costs.append(cost_record)

        # Log cost information
        self.logger.debug(
            f"Task cost: ${task_cost:.6f} "
            f"({input_tokens} input + {output_tokens} output tokens) "
            f"- {self.current_operation} - {self.provider}/{self.model}"
        )

    def get_cost(self) -> float:
        """
        Calculate total cost so far.

        Returns:
            Total cost in dollars across all operations
        """
        return self.total_cost

    def get_stats(self) -> Dict[str, Any]:
        """Get usage statistics including fallback and model selection information"""
        stats = {
            'provider': self.provider,
            'model': self.model,
            'input_tokens': self.input_tokens,
            'output_tokens': self.output_tokens,
            'total_tokens': self.input_tokens + self.output_tokens,
            'cost': self.get_cost(),
            'input_price_per_1m': self.config['input_price'] * 1_000_000,
            'output_price_per_1m': self.config['output_price'] * 1_000_000,
            'fallback_attempts': self.fallback_attempts,
            'failed_providers': self.failed_providers,
            'enable_fallback': self.enable_fallback,
            'enable_model_selection': self.enable_model_selection
        }

        # Add model selector performance if available
        if self.model_selector:
            stats['model_selection'] = self.model_selector.get_performance_report()

        return stats

    def reset_stats(self):
        """Reset token tracking and fallback statistics"""
        self.input_tokens = 0
        self.output_tokens = 0
        self.fallback_attempts = 0
        self.failed_providers = []
        self.total_cost = 0.0
        self.task_costs = []

    def _set_operation(self, operation: str):
        """
        Set current operation for cost tracking context.

        Args:
            operation: Name of the operation being performed
        """
        self.current_operation = operation
        self.operation_start_time = time.time()

    def _costs_by_provider(self) -> Dict[str, Dict[str, Any]]:
        """
        Aggregate costs grouped by provider.

        Returns:
            Dictionary with provider names as keys and cost details as values
        """
        provider_costs = {}

        for task in self.task_costs:
            provider = task.provider
            if provider not in provider_costs:
                provider_costs[provider] = {
                    'total_cost': 0.0,
                    'input_tokens': 0,
                    'output_tokens': 0,
                    'task_count': 0,
                    'operations': {}
                }

            pc = provider_costs[provider]
            pc['total_cost'] += task.total_cost
            pc['input_tokens'] += task.input_tokens
            pc['output_tokens'] += task.output_tokens
            pc['task_count'] += 1

            # Track by operation type
            op = task.operation
            if op not in pc['operations']:
                pc['operations'][op] = {
                    'count': 0,
                    'total_cost': 0.0,
                    'avg_latency_ms': 0.0
                }

            pc['operations'][op]['count'] += 1
            pc['operations'][op]['total_cost'] += task.total_cost

            # Update average latency
            op_stats = pc['operations'][op]
            prev_count = op_stats['count'] - 1
            if prev_count > 0:
                prev_avg = op_stats['avg_latency_ms']
                op_stats['avg_latency_ms'] = (
                    (prev_avg * prev_count + task.latency_ms) / op_stats['count']
                )
            else:
                op_stats['avg_latency_ms'] = task.latency_ms

        # Round all costs for cleaner output
        for provider, data in provider_costs.items():
            data['total_cost'] = round(data['total_cost'], 6)
            for op_data in data['operations'].values():
                op_data['total_cost'] = round(op_data['total_cost'], 6)
                op_data['avg_latency_ms'] = round(op_data['avg_latency_ms'], 2)

        return provider_costs

    def get_cost_summary(self) -> Dict[str, Any]:
        """
        Export comprehensive cost tracking data.

        Returns:
            Dictionary containing:
            - total_cost: Total cost across all tasks
            - task_count: Number of tasks executed
            - average_cost: Average cost per task
            - by_provider: Costs grouped by provider
            - by_operation: Costs grouped by operation type
            - tasks: List of individual task costs
            - efficiency_metrics: Cost efficiency indicators
        """
        task_count = len(self.task_costs)

        # Calculate costs by operation
        operation_costs = {}
        for task in self.task_costs:
            op = task.operation
            if op not in operation_costs:
                operation_costs[op] = {
                    'count': 0,
                    'total_cost': 0.0,
                    'avg_cost': 0.0,
                    'total_tokens': 0,
                    'avg_latency_ms': 0.0
                }

            op_stats = operation_costs[op]
            op_stats['count'] += 1
            op_stats['total_cost'] += task.total_cost
            op_stats['total_tokens'] += task.input_tokens + task.output_tokens

            # Update running average for latency
            prev_count = op_stats['count'] - 1
            if prev_count > 0:
                prev_avg = op_stats['avg_latency_ms']
                op_stats['avg_latency_ms'] = (
                    (prev_avg * prev_count + task.latency_ms) / op_stats['count']
                )
            else:
                op_stats['avg_latency_ms'] = task.latency_ms

        # Calculate averages
        for op_stats in operation_costs.values():
            if op_stats['count'] > 0:
                op_stats['avg_cost'] = op_stats['total_cost'] / op_stats['count']
            op_stats['total_cost'] = round(op_stats['total_cost'], 6)
            op_stats['avg_cost'] = round(op_stats['avg_cost'], 6)
            op_stats['avg_latency_ms'] = round(op_stats['avg_latency_ms'], 2)

        # Calculate efficiency metrics
        total_tokens = self.input_tokens + self.output_tokens
        efficiency_metrics = {
            'cost_per_1k_tokens': round(
                (self.total_cost / total_tokens * 1000) if total_tokens > 0 else 0, 6
            ),
            'avg_tokens_per_task': round(
                total_tokens / task_count if task_count > 0 else 0, 2
            ),
            'input_output_ratio': round(
                self.input_tokens / self.output_tokens if self.output_tokens > 0 else 0, 2
            ),
            'total_input_tokens': self.input_tokens,
            'total_output_tokens': self.output_tokens,
            'total_tokens': total_tokens
        }

        return {
            'total_cost': round(self.total_cost, 6),
            'task_count': task_count,
            'average_cost': round(self.total_cost / task_count if task_count > 0 else 0, 6),
            'by_provider': self._costs_by_provider(),
            'by_operation': operation_costs,
            'efficiency_metrics': efficiency_metrics,
            'tasks': [task.to_dict() for task in self.task_costs]
        }

    def orchestrate(self, task: str, capabilities: List[str] = None,
                   context: Dict[str, Any] = None) -> str:
        """
        End-to-end orchestration method that was missing.

        Coordinates the complete workflow:
        1. Route to specialized FREE model if enabled (NEW - Phase 2)
        2. Analyze and plan the task
        3. For simple tasks: execute directly
        4. For complex tasks: decompose, execute, validate, assemble

        Args:
            task: Task description
            capabilities: Optional list of required capabilities
            context: Optional context information

        Returns:
            Final result as string
        """
        # Default values
        if capabilities is None:
            capabilities = ['research', 'analysis', 'documentation', 'code generation']
        if context is None:
            context = {}

        # Phase 0: Specialized Model Routing (NEW - Phase 2)
        if self.specialized_router:
            model_spec, task_type, reasoning = self.specialized_router.route(task, prefer_free=True)
            self.logger.info(f"Specialized routing: {reasoning}")

            # Override provider/model if router suggests a better match
            if model_spec.is_free:  # Only use FREE models from router
                original_provider = self.provider
                original_model = self.model

                try:
                    # Temporarily switch to specialized model
                    self.logger.info(f"Switching from {self.provider}/{self.model} to specialized FREE model: {model_spec.model_id}")
                    self.provider = 'openrouter'  # All specialized models via OpenRouter
                    self.model = model_spec.model_id
                    self._init_client()
                except Exception as e:
                    self.logger.warning(f"Failed to switch to specialized model: {e}. Using standard routing.")
                    self.provider = original_provider
                    self.model = original_model
                    self._init_client()

        # Phase 1: Analyze and Plan
        plan = self.analyze_and_plan(task, capabilities, context)

        # Phase 2: Execute based on complexity
        if not plan.decompose or len(plan.subtasks) == 0:
            # Simple task - execute directly with orchestrator
            prompt = f"""Task: {task}

Context: {json.dumps(context, indent=2)}

Please provide a comprehensive response."""

            result = self._call_llm_with_retry(prompt, max_tokens=16000)
            return result

        # Complex task - use worker delegation
        # Define available workers (cheap models)
        available_workers = {
            'deepseek_v3': {
                'model': 'deepseek/deepseek-chat',
                'price': '$0.0005/1K',
                'quality': 85,
                'speed': 'fast',
                'best_for': 'code analysis, technical documentation',
                'weaknesses': 'may struggle with complex reasoning, newer model with less testing'
            },
            'gpt4o_mini': {
                'model': 'gpt-4o-mini',
                'price': '$0.00015/1K input, $0.0006/1K output',
                'quality': 90,
                'speed': 'very fast',
                'best_for': 'general tasks, analysis',
                'weaknesses': 'smaller context window than full models, less nuanced responses'
            }
        }

        # Phase 3: Assign workers
        assignments = self.assign_workers(plan.subtasks, available_workers)

        # Phase 4: Execute subtasks
        results = {}
        for subtask in plan.subtasks:
            worker_name = assignments.get(subtask.id)
            if not worker_name:
                continue

            worker_info = available_workers[worker_name]

            # Generate detailed specs for this subtask
            specs = self.generate_specs(subtask, worker_info, context)

            # Execute with the worker (for now, use main LLM)
            # TODO: In production, route to actual cheap worker APIs
            prompt = f"""You are a specialized worker executing a subtask.

Subtask: {subtask.description}

Detailed specifications:
{specs}

Original task context:
{json.dumps(context, indent=2)}

Please complete this subtask thoroughly."""

            result = self._call_llm_with_retry(prompt, max_tokens=8000)
            results[subtask.id] = result

        # Phase 5: Validate outputs
        validation = self.validate_outputs(results, plan)

        # Phase 6: Assemble and polish
        final_result = self.assemble_and_polish(results, plan)

        return final_result
